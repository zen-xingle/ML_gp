
----------------------------------------

  Demo lar cigp 
  seed: None 
  interp_data: True 

----------------------------------------
---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': None, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: None
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00529     	0.01201     	0.98425     	0           
  10          	0.00481     	0.01106     	0.98662     	0           
  100         	0.00442     	0.01016     	0.98884     	0           
  300         	0.00413     	0.00963     	0.99009     	1           
  500         	0.00398     	0.00931     	0.99081     	2           
  1000        	0.00376     	0.00882     	0.99186     	4           
---> try to load best state
              	0.00376     	0.00882     	0.99186     	4                eval state : test_on_restore;
              	0.00376     	0.00882     	0.99186     	4                eval state : test_on_last_epoch;
---> final result
              	0.00376     	0.00882     	0.99186     	4                eval state : final;
----------> finish x-yl training


---------->
lar cigp for 4 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 4, 'eval_start_index': 0, 'eval_sample': 128, 'seed': None, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.02799     	0.05985     	0.73567     	0           
  10          	0.04255     	0.07439     	0.53597     	0           
  100         	0.04073     	0.07290     	0.61481     	0           
  300         	0.04085     	0.07311     	0.61630     	0           
  500         	0.04099     	0.07332     	0.61651     	1           
  1000        	0.04122     	0.07365     	0.61691     	2           
---> try to load best state
              	0.04122     	0.07365     	0.61691     	2                eval state : test_on_restore;
              	0.04122     	0.07365     	0.61691     	2                eval state : test_on_last_epoch;
---> final result
              	0.04122     	0.07365     	0.61691     	2                eval state : final;

---------->
lar cigp for 8 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 8, 'eval_start_index': 0, 'eval_sample': 128, 'seed': None, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01647     	0.04453     	0.86117     	0           
  10          	0.01705     	0.04419     	0.86268     	0           
  100         	0.01534     	0.04195     	0.87422     	0           
  300         	0.01533     	0.04183     	0.87477     	0           
  500         	0.01534     	0.04176     	0.87511     	1           
  1000        	0.01536     	0.04168     	0.87543     	2           
---> try to load best state
              	0.01536     	0.04168     	0.87543     	2                eval state : test_on_restore;
              	0.01536     	0.04168     	0.87543     	2                eval state : test_on_last_epoch;
---> final result
              	0.01536     	0.04168     	0.87543     	2                eval state : final;

---------->
lar cigp for 16 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128, 'seed': None, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01032     	0.03162     	0.91791     	0           
  10          	0.01084     	0.03133     	0.91844     	0           
  100         	0.01057     	0.03065     	0.92187     	0           
  300         	0.01035     	0.03046     	0.92306     	1           
  500         	0.01020     	0.03031     	0.92395     	1           
  1000        	0.01002     	0.03005     	0.92537     	3           
---> try to load best state
              	0.01002     	0.03005     	0.92537     	3                eval state : test_on_restore;
              	0.01002     	0.03005     	0.92537     	3                eval state : test_on_last_epoch;
---> final result
              	0.01002     	0.03005     	0.92537     	3                eval state : final;

---------->
lar cigp for 32 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': None, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00745     	0.02376     	0.94957     	0           
  10          	0.00770     	0.02433     	0.94893     	0           
  100         	0.00763     	0.02407     	0.95016     	0           
  300         	0.00752     	0.02382     	0.95099     	1           
  500         	0.00746     	0.02365     	0.95152     	2           
  1000        	0.00738     	0.02342     	0.95224     	5           
---> try to load best state
              	0.00738     	0.02342     	0.95224     	5                eval state : test_on_restore;
              	0.00738     	0.02342     	0.95224     	5                eval state : test_on_last_epoch;
---> final result
              	0.00738     	0.02342     	0.95224     	5                eval state : final;
----------------------------------------

  Demo lar cigp 
  seed: 1 
  interp_data: True 

----------------------------------------
---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 1, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: None
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00742     	0.02005     	0.95349     	0           
  10          	0.00666     	0.01731     	0.96124     	0           
  100         	0.00598     	0.01526     	0.96835     	0           
  300         	0.00561     	0.01445     	0.97167     	1           
  500         	0.00536     	0.01402     	0.97369     	2           
  1000        	0.00494     	0.01342     	0.97668     	4           
---> try to load best state
              	0.00494     	0.01342     	0.97668     	4                eval state : test_on_restore;
              	0.00494     	0.01342     	0.97668     	4                eval state : test_on_last_epoch;
---> final result
              	0.00494     	0.01342     	0.97668     	4                eval state : final;
----------> finish x-yl training


---------->
lar cigp for 4 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 4, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 1, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01945     	0.03982     	0.61027     	0           
  10          	0.03592     	0.07336     	-1.15838    	0           
  100         	0.03753     	0.07472     	-0.71830    	0           
  300         	0.03793     	0.07535     	-0.74844    	0           
  500         	0.03795     	0.07538     	-0.74452    	1           
  1000        	0.03797     	0.07543     	-0.73856    	2           
---> try to load best state
              	0.03797     	0.07543     	-0.73856    	2                eval state : test_on_restore;
              	0.03797     	0.07543     	-0.73856    	2                eval state : test_on_last_epoch;
---> final result
              	0.03797     	0.07543     	-0.73856    	2                eval state : final;

---------->
lar cigp for 8 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 8, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 1, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01602     	0.03288     	0.68449     	0           
  10          	0.02340     	0.04142     	0.39669     	0           
  100         	0.01684     	0.03092     	0.67321     	0           
  300         	0.01655     	0.03070     	0.68721     	0           
  500         	0.01632     	0.03050     	0.69768     	1           
  1000        	0.01588     	0.03008     	0.71608     	2           
---> try to load best state
              	0.01588     	0.03008     	0.71608     	2                eval state : test_on_restore;
              	0.01588     	0.03008     	0.71608     	2                eval state : test_on_last_epoch;
---> final result
              	0.01588     	0.03008     	0.71608     	2                eval state : final;

---------->
lar cigp for 16 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 1, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00849     	0.02544     	0.86607     	0           
  10          	0.00920     	0.02695     	0.86355     	0           
  100         	0.00787     	0.02328     	0.89690     	0           
  300         	0.00783     	0.02310     	0.89979     	1           
  500         	0.00781     	0.02306     	0.90084     	1           
  1000        	0.00776     	0.02301     	0.90246     	3           
---> try to load best state
              	0.00776     	0.02301     	0.90246     	3                eval state : test_on_restore;
              	0.00776     	0.02301     	0.90246     	3                eval state : test_on_last_epoch;
---> final result
              	0.00776     	0.02301     	0.90246     	3                eval state : final;

---------->
lar cigp for 32 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 1, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00708     	0.02134     	0.92318     	0           
  10          	0.00737     	0.02146     	0.92302     	0           
  100         	0.00721     	0.02093     	0.93070     	0           
  300         	0.00713     	0.02075     	0.93287     	1           
  500         	0.00707     	0.02065     	0.93406     	2           
  1000        	0.00700     	0.02050     	0.93582     	5           
---> try to load best state
              	0.00700     	0.02050     	0.93582     	5                eval state : test_on_restore;
              	0.00700     	0.02050     	0.93582     	5                eval state : test_on_last_epoch;
---> final result
              	0.00700     	0.02050     	0.93582     	5                eval state : final;
----------------------------------------

  Demo lar cigp 
  seed: 2 
  interp_data: True 

----------------------------------------
---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 2, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: None
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01016     	0.03130     	0.90580     	0           
  10          	0.00811     	0.02458     	0.92401     	0           
  100         	0.00709     	0.02076     	0.93947     	0           
  300         	0.00671     	0.01949     	0.94572     	1           
  500         	0.00648     	0.01892     	0.94902     	1           
  1000        	0.00610     	0.01818     	0.95409     	3           
---> try to load best state
              	0.00610     	0.01818     	0.95409     	3                eval state : test_on_restore;
              	0.00610     	0.01818     	0.95409     	3                eval state : test_on_last_epoch;
---> final result
              	0.00610     	0.01818     	0.95409     	3                eval state : final;
----------> finish x-yl training


---------->
lar cigp for 4 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 4, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 2, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.03052     	0.05364     	0.18679     	0           
  10          	0.05989     	0.10111     	-12.48023   	0           
  100         	0.05977     	0.10113     	-16.80450   	0           
  300         	0.06903     	0.11417     	-32.97603   	0           
  500         	0.06941     	0.11467     	-33.39589   	1           
  1000        	0.06890     	0.11391     	-30.57714   	2           
---> try to load best state
              	0.06890     	0.11391     	-30.57714   	2                eval state : test_on_restore;
              	0.06890     	0.11391     	-30.57714   	2                eval state : test_on_last_epoch;
---> final result
              	0.06890     	0.11391     	-30.57714   	2                eval state : final;

---------->
lar cigp for 8 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 8, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 2, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.02005     	0.04509     	0.23810     	0           
  10          	0.03609     	0.08087     	-1.97667    	0           
  100         	0.02151     	0.05410     	-0.14627    	0           
  300         	0.01957     	0.05041     	0.01755     	0           
  500         	0.01892     	0.04916     	0.07616     	1           
  1000        	0.01840     	0.04813     	0.13171     	2           
---> try to load best state
              	0.01840     	0.04813     	0.13171     	2                eval state : test_on_restore;
              	0.01840     	0.04813     	0.13171     	2                eval state : test_on_last_epoch;
---> final result
              	0.01840     	0.04813     	0.13171     	2                eval state : final;

---------->
lar cigp for 16 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 2, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00969     	0.02945     	0.81218     	0           
  10          	0.01066     	0.03141     	0.81348     	0           
  100         	0.00855     	0.02630     	0.86989     	0           
  300         	0.00851     	0.02616     	0.87284     	1           
  500         	0.00848     	0.02610     	0.87431     	1           
  1000        	0.00842     	0.02599     	0.87708     	3           
---> try to load best state
              	0.00842     	0.02599     	0.87708     	3                eval state : test_on_restore;
              	0.00842     	0.02599     	0.87708     	3                eval state : test_on_last_epoch;
---> final result
              	0.00842     	0.02599     	0.87708     	3                eval state : final;

---------->
lar cigp for 32 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 2, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00912     	0.02698     	0.88614     	0           
  10          	0.00900     	0.02559     	0.90379     	0           
  100         	0.00904     	0.02559     	0.90487     	0           
  300         	0.00904     	0.02558     	0.90492     	1           
  500         	0.00904     	0.02558     	0.90492     	2           
  1000        	0.00904     	0.02558     	0.90493     	5           
---> try to load best state
              	0.00904     	0.02558     	0.90493     	5                eval state : test_on_restore;
              	0.00904     	0.02558     	0.90493     	5                eval state : test_on_last_epoch;
---> final result
              	0.00904     	0.02558     	0.90493     	5                eval state : final;
----------------------------------------

  Demo lar cigp 
  seed: 3 
  interp_data: True 

----------------------------------------
---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 3, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: None
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00603     	0.01658     	0.96828     	0           
  10          	0.00522     	0.01423     	0.97483     	0           
  100         	0.00452     	0.01163     	0.98142     	0           
  300         	0.00431     	0.01087     	0.98373     	1           
  500         	0.00421     	0.01058     	0.98472     	1           
  1000        	0.00402     	0.01025     	0.98596     	3           
---> try to load best state
              	0.00402     	0.01025     	0.98596     	3                eval state : test_on_restore;
              	0.00402     	0.01025     	0.98596     	3                eval state : test_on_last_epoch;
---> final result
              	0.00402     	0.01025     	0.98596     	3                eval state : final;
----------> finish x-yl training


---------->
lar cigp for 4 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 4, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 3, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01273     	0.02963     	0.73720     	0           
  10          	0.02057     	0.04746     	-0.05786    	0           
  100         	0.01650     	0.03913     	0.33633     	0           
  300         	0.01638     	0.03890     	0.34501     	0           
  500         	0.01634     	0.03883     	0.34816     	1           
  1000        	0.01628     	0.03871     	0.35310     	2           
---> try to load best state
              	0.01628     	0.03871     	0.35310     	2                eval state : test_on_restore;
              	0.01628     	0.03871     	0.35310     	2                eval state : test_on_last_epoch;
---> final result
              	0.01628     	0.03871     	0.35310     	2                eval state : final;

---------->
lar cigp for 8 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 8, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 3, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01068     	0.02861     	0.76409     	0           
  10          	0.01468     	0.03910     	0.59597     	0           
  100         	0.00854     	0.02538     	0.81050     	0           
  300         	0.00838     	0.02490     	0.82075     	0           
  500         	0.00829     	0.02462     	0.82693     	1           
  1000        	0.00817     	0.02417     	0.83652     	2           
---> try to load best state
              	0.00817     	0.02417     	0.83652     	2                eval state : test_on_restore;
              	0.00817     	0.02417     	0.83652     	2                eval state : test_on_last_epoch;
---> final result
              	0.00817     	0.02417     	0.83652     	2                eval state : final;

---------->
lar cigp for 16 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 3, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00722     	0.02208     	0.88476     	0           
  10          	0.00843     	0.02623     	0.85539     	0           
  100         	0.00687     	0.02041     	0.90625     	0           
  300         	0.00684     	0.02011     	0.91077     	1           
  500         	0.00682     	0.01998     	0.91279     	1           
  1000        	0.00676     	0.01980     	0.91578     	3           
---> try to load best state
              	0.00676     	0.01980     	0.91578     	3                eval state : test_on_restore;
              	0.00676     	0.01980     	0.91578     	3                eval state : test_on_last_epoch;
---> final result
              	0.00676     	0.01980     	0.91578     	3                eval state : final;

---------->
lar cigp for 32 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 3, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00620     	0.01562     	0.96656     	0           
  10          	0.00617     	0.01496     	0.97041     	0           
  100         	0.00621     	0.01514     	0.96971     	0           
  300         	0.00616     	0.01511     	0.97000     	1           
  500         	0.00610     	0.01509     	0.97019     	2           
  1000        	0.00600     	0.01506     	0.97045     	5           
---> try to load best state
              	0.00600     	0.01506     	0.97045     	5                eval state : test_on_restore;
              	0.00600     	0.01506     	0.97045     	5                eval state : test_on_last_epoch;
---> final result
              	0.00600     	0.01506     	0.97045     	5                eval state : final;
----------------------------------------

  Demo lar cigp 
  seed: 4 
  interp_data: True 

----------------------------------------
---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 4, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: None
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00909     	0.01980     	0.96227     	0           
  10          	0.00800     	0.01754     	0.96856     	0           
  100         	0.00694     	0.01514     	0.97480     	0           
  300         	0.00658     	0.01457     	0.97728     	1           
  500         	0.00649     	0.01450     	0.97831     	1           
  1000        	0.00648     	0.01459     	0.97924     	3           
---> try to load best state
              	0.00648     	0.01459     	0.97924     	3                eval state : test_on_restore;
              	0.00648     	0.01459     	0.97924     	3                eval state : test_on_last_epoch;
---> final result
              	0.00648     	0.01459     	0.97924     	3                eval state : final;
----------> finish x-yl training


---------->
lar cigp for 4 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 4, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 4, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.03100     	0.04821     	0.55049     	0           
  10          	0.06580     	0.08840     	-5.99671    	0           
  100         	0.07626     	0.10221     	-14.11384   	0           
  300         	0.07634     	0.10232     	-14.21999   	0           
  500         	0.07634     	0.10232     	-14.21999   	1           
  1000        	0.07634     	0.10232     	-14.21999   	2           
---> try to load best state
              	0.07634     	0.10232     	-14.21999   	2                eval state : test_on_restore;
              	0.07634     	0.10232     	-14.21999   	2                eval state : test_on_last_epoch;
---> final result
              	0.07634     	0.10232     	-14.21999   	2                eval state : final;

---------->
lar cigp for 8 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 8, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 4, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01721     	0.02807     	0.90182     	0           
  10          	0.01597     	0.02604     	0.91631     	0           
  100         	0.01592     	0.02577     	0.91959     	0           
  300         	0.01585     	0.02565     	0.91963     	0           
  500         	0.01583     	0.02562     	0.91962     	1           
  1000        	0.01583     	0.02561     	0.91962     	2           
---> try to load best state
              	0.01583     	0.02561     	0.91962     	2                eval state : test_on_restore;
              	0.01583     	0.02561     	0.91962     	2                eval state : test_on_last_epoch;
---> final result
              	0.01583     	0.02561     	0.91962     	2                eval state : final;

---------->
lar cigp for 16 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 4, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01645     	0.03148     	0.89247     	0           
  10          	0.01662     	0.03180     	0.88935     	0           
  100         	0.01663     	0.03181     	0.88925     	0           
  300         	0.01670     	0.03195     	0.88790     	1           
  500         	0.01673     	0.03202     	0.88703     	1           
  1000        	0.01676     	0.03210     	0.88592     	3           
---> try to load best state
              	0.01676     	0.03210     	0.88592     	3                eval state : test_on_restore;
              	0.01676     	0.03210     	0.88592     	3                eval state : test_on_last_epoch;
---> final result
              	0.01676     	0.03210     	0.88592     	3                eval state : final;

---------->
lar cigp for 32 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 4, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01002     	0.02298     	0.93934     	0           
  10          	0.01018     	0.02321     	0.93825     	0           
  100         	0.01022     	0.02330     	0.93772     	0           
  300         	0.01021     	0.02319     	0.93822     	1           
  500         	0.01015     	0.02303     	0.93897     	2           
  1000        	0.00997     	0.02266     	0.94080     	5           
---> try to load best state
              	0.00997     	0.02266     	0.94080     	5                eval state : test_on_restore;
              	0.00997     	0.02266     	0.94080     	5                eval state : test_on_last_epoch;
---> final result
              	0.00997     	0.02266     	0.94080     	5                eval state : final;
----------------------------------------

  Demo lar cigp 
  seed: 0 
  interp_data: True 

----------------------------------------
---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 0, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: None
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01120     	0.02957     	0.89706     	0           
  10          	0.00913     	0.02337     	0.92935     	0           
  100         	0.00752     	0.01979     	0.94638     	0           
  300         	0.00700     	0.01867     	0.95149     	1           
  500         	0.00671     	0.01814     	0.95399     	1           
  1000        	0.00626     	0.01743     	0.95734     	3           
---> try to load best state
              	0.00626     	0.01743     	0.95734     	3                eval state : test_on_restore;
              	0.00626     	0.01743     	0.95734     	3                eval state : test_on_last_epoch;
---> final result
              	0.00626     	0.01743     	0.95734     	3                eval state : final;
----------> finish x-yl training


---------->
lar cigp for 4 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 4, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 0, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.02182     	0.04300     	0.38920     	0           
  10          	0.03541     	0.06185     	-0.49271    	0           
  100         	0.02758     	0.05097     	-0.03969    	0           
  300         	0.02727     	0.05046     	-0.01398    	0           
  500         	0.02700     	0.05003     	0.00727     	1           
  1000        	0.02654     	0.04928     	0.04351     	1           
---> try to load best state
              	0.02654     	0.04928     	0.04351     	1                eval state : test_on_restore;
              	0.02654     	0.04928     	0.04351     	2                eval state : test_on_last_epoch;
---> final result
              	0.02654     	0.04928     	0.04351     	2                eval state : final;

---------->
lar cigp for 8 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 8, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 0, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.01451     	0.03594     	0.63874     	0           
  10          	0.01827     	0.04458     	0.48519     	0           
  100         	0.01191     	0.03343     	0.68534     	0           
  300         	0.01179     	0.03319     	0.69159     	0           
  500         	0.01170     	0.03300     	0.69631     	1           
  1000        	0.01154     	0.03269     	0.70427     	2           
---> try to load best state
              	0.01154     	0.03269     	0.70427     	2                eval state : test_on_restore;
              	0.01154     	0.03269     	0.70427     	2                eval state : test_on_last_epoch;
---> final result
              	0.01154     	0.03269     	0.70427     	2                eval state : final;

---------->
lar cigp for 16 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 0, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00948     	0.02630     	0.86448     	0           
  10          	0.00945     	0.02644     	0.86456     	0           
  100         	0.00851     	0.02516     	0.87734     	0           
  300         	0.00846     	0.02494     	0.88226     	0           
  500         	0.00846     	0.02479     	0.88564     	1           
  1000        	0.00846     	0.02455     	0.89109     	2           
---> try to load best state
              	0.00846     	0.02455     	0.89109     	2                eval state : test_on_restore;
              	0.00846     	0.02455     	0.89109     	2                eval state : test_on_last_epoch;
---> final result
              	0.00846     	0.02455     	0.89109     	2                eval state : final;

---------->
lar cigp for 32 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Heat_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'train_start_index': 0, 'train_sample': 32, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 0, 'interp_data': True}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': False, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 100.0
  res_cigp: {'type_name': 'res_rho'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.00850     	0.02597     	0.86588     	0           
  10          	0.00882     	0.02619     	0.86702     	0           
  100         	0.00829     	0.02507     	0.88278     	0           
  300         	0.00825     	0.02489     	0.88690     	1           
  500         	0.00823     	0.02480     	0.88919     	2           
  1000        	0.00819     	0.02465     	0.89273     	4           
---> try to load best state
              	0.00819     	0.02465     	0.89273     	4                eval state : test_on_restore;
              	0.00819     	0.02465     	0.89273     	4                eval state : test_on_last_epoch;
---> final result
              	0.00819     	0.02465     	0.89273     	4                eval state : final;
----------------------------------------

  Demo hogp 
  seed: None 
  interp_data: False 

----------------------------------------
----------------------------------------

  Demo hogp 
  seed: 0 
  interp_data: False 

----------------------------------------
----------------------------------------

  Demo hogp 
  seed: 1 
  interp_data: False 

----------------------------------------
---> module config
  dataset: {'name': 'TopOP_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 64, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 1, 'interp_data': False}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalzie: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.20929     	0.25268     	-61.67487   	0           
  10          	0.07344     	0.10705     	0.64179     	0           
  100         	0.05476     	0.08241     	0.85579     	0           
  300         	0.04911     	0.07437     	0.89309     	1           
  500         	0.04591     	0.06997     	0.91006     	2           
  1000        	0.04089     	0.06328     	0.93172     	5           
---> try to load best state
              	0.04089     	0.06328     	0.93172     	5                eval state : test_on_restore;
              	0.04089     	0.06328     	0.93172     	5                eval state : test_on_last_epoch;
---> final result              	0.04089     	0.06328     	0.93172     	5                eval state : final;
---> end

----------------------------------------

  Demo hogp 
  seed: None 
  interp_data: False 

----------------------------------------
---> module config
  dataset: {'name': 'TopOP_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 64, 'eval_start_index': 0, 'eval_sample': 128, 'seed': None, 'interp_data': False}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalzie: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.20929     	0.25268     	-61.67487   	0           
  10          	0.07344     	0.10705     	0.64179     	0           
  100         	0.05476     	0.08241     	0.85579     	0           
  300         	0.04911     	0.07437     	0.89309     	1           
  500         	0.04591     	0.06997     	0.91006     	2           
  1000        	0.04089     	0.06328     	0.93172     	5           
---> try to load best state
              	0.04089     	0.06328     	0.93172     	5                eval state : test_on_restore;
              	0.04089     	0.06328     	0.93172     	5                eval state : test_on_last_epoch;
---> final result              	0.04089     	0.06328     	0.93172     	5                eval state : final;
---> end

----------------------------------------

  Demo hogp 
  seed: 0 
  interp_data: False 

----------------------------------------
---> module config
  dataset: {'name': 'TopOP_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 64, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 0, 'interp_data': False}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalzie: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.20929     	0.25268     	-61.67487   	0           
  10          	0.07344     	0.10705     	0.64179     	0           
  100         	0.05476     	0.08241     	0.85579     	0           
  300         	0.04911     	0.07437     	0.89309     	1           
  500         	0.04591     	0.06997     	0.91006     	2           
  1000        	0.04089     	0.06328     	0.93172     	5           
---> try to load best state
              	0.04089     	0.06328     	0.93172     	5                eval state : test_on_restore;
              	0.04089     	0.06328     	0.93172     	5                eval state : test_on_last_epoch;
---> final result              	0.04089     	0.06328     	0.93172     	5                eval state : final;
---> end

----------------------------------------

  Demo hogp 
  seed: 1 
  interp_data: False 

----------------------------------------
---> module config
  dataset: {'name': 'TopOP_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 64, 'eval_start_index': 0, 'eval_sample': 128, 'seed': 1, 'interp_data': False}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalzie: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.20929     	0.25268     	-61.67487   	0           
  10          	0.07344     	0.10705     	0.64179     	0           
  100         	0.05476     	0.08241     	0.85579     	0           
  300         	0.04911     	0.07437     	0.89309     	1           
----------------------------------------

  Demo GAR 
  seed: None 
  interp_data: True 

----------------------------------------
---> Training x -> yl part
---> module config
  dataset: {'name': 'Burget_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 128, 'eval_start_index': 0, 'eval_sample': 128}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalzie: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.16775     	0.21728     	-0.23626    	0           
  10          	0.03005     	0.04688     	0.98255     	0           
  100         	0.01688     	0.03597     	0.98835     	1           
  300         	0.01331     	0.02624     	0.99341     	3           
  500         	0.01137     	0.02227     	0.99515     	5           
  1000        	0.00920     	0.01865     	0.99652     	10          
---> try to load best state
              	0.00920     	0.01865     	0.99652     	10               eval state : test_on_restore;
              	0.00920     	0.01865     	0.99652     	10               eval state : test_on_last_epoch;
---> final result              	0.00920     	0.01865     	0.99652     	10               eval state : final;
---> end


---------->
SGAR for 4 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Burget_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'connection_method': 'res_mapping', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128}
  lr: {'kernel': 0.01, 'optional_param': 0.01, 'noise': 0.01}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.06829     	0.13615     	0.79189     	0           
  10          	0.02362     	0.04660     	0.97661     	0           
  100         	0.67826     	0.76595     	0.26428     	0           
  epoch 196 reach nan state
---> try to load best state
              	0.02613     	0.03621     	0.98806     	1                eval state : test_on_restore;
              	0.67826     	0.76595     	0.26428     	1                eval state : test_on_last_epoch;
---> final result              	0.02613     	0.03621     	0.98806     	1                eval state : final;
---> end

----------------------------------------

  Demo GAR 
  seed: None 
  interp_data: True 

----------------------------------------
---> Training x -> yl part
---> module config
  dataset: {'name': 'Burget_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 128, 'eval_start_index': 0, 'eval_sample': 128}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalzie: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.16775     	0.21728     	-0.23626    	0           
  10          	0.03005     	0.04688     	0.98255     	0           
  100         	0.01688     	0.03597     	0.98835     	1           
  300         	0.01331     	0.02624     	0.99341     	3           
  500         	0.01137     	0.02227     	0.99515     	5           
  1000        	0.00920     	0.01865     	0.99652     	10          
---> try to load best state
              	0.00920     	0.01865     	0.99652     	10               eval state : test_on_restore;
              	0.00920     	0.01865     	0.99652     	10               eval state : test_on_last_epoch;
---> final result              	0.00920     	0.01865     	0.99652     	10               eval state : final;
---> end


---------->
SGAR for 4 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Burget_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'connection_method': 'res_mapping', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128}
  lr: {'kernel': 0.01, 'optional_param': 0.01, 'noise': 0.01}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.06829     	0.13615     	0.79189     	0           
  10          	0.02362     	0.04660     	0.97661     	0           
  100         	0.67826     	0.76595     	0.26428     	0           
  epoch 196 reach nan state
---> try to load best state
              	0.02613     	0.03621     	0.98806     	1                eval state : test_on_restore;
              	0.67826     	0.76595     	0.26428     	1                eval state : test_on_last_epoch;
---> final result              	0.02613     	0.03621     	0.98806     	1                eval state : final;
---> end


---------->
SGAR for 8 samples
---> Training x,yl -> yh part

---> module config
  dataset: {'name': 'Burget_mfGent_v5', 'fidelity': ['low', 'high'], 'type': 'x_yl_2_yh', 'connection_method': 'res_mapping', 'train_start_index': 0, 'train_sample': 16, 'eval_start_index': 0, 'eval_sample': 128}
  lr: {'kernel': 0.01, 'optional_param': 0.01, 'noise': 0.01}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalize: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.06829     	0.13615     	0.79189     	0           
  10          	0.02362     	0.04660     	0.97661     	0           
  100         	0.67826     	0.76595     	0.26428     	1           
  epoch 196 reach nan state
---> try to load best state
              	0.02613     	0.03621     	0.98806     	2                eval state : test_on_restore;
              	0.67826     	0.76595     	0.26428     	2                eval state : test_on_last_epoch;
---> final result              	0.02613     	0.03621     	0.98806     	2                eval state : final;
---> end

----------------------------------------

  Demo GAR 
  seed: 0 
  interp_data: True 

----------------------------------------
---> Training x -> yl part
---> module config
  dataset: {'name': 'Burget_mfGent_v5', 'fidelity': ['low'], 'type': 'x_2_y', 'train_start_index': 0, 'train_sample': 128, 'eval_start_index': 0, 'eval_sample': 128}
  lr: {'kernel': 0.1, 'optional_param': 0.1, 'noise': 0.1}
  kernel: {'K1': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K2': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}, 'K3': {'SE': {'exp_restrict': True, 'length_scale': 1.0, 'scale': 1.0}}}
  evaluate_method: ['mae', 'rmse', 'r2']
  optimizer: adam
  exp_restrict: False
  input_normalzie: True
  output_normalize: True
  noise_init: 0.005
  grid_config: {'grid_size': [-1, -1], 'type': 'fixed', 'dimension_map': 'identity'}
---> training record
  epoch       	mae         	rmse        	r2          	time        
  1           	0.16775     	0.21728     	-0.23626    	0           
  10          	0.03005     	0.04688     	0.98255     	0           
  100         	0.01688     	0.03597     	0.98835     	1           
  300         	0.01331     	0.02624     	0.99341     	3           
